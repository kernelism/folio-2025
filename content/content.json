{
  "header": {
    "name": "Arjhun Sreedar",
    "nameAudio": "ur-june shree-dhar",
    "tagline": "I solve problems with code",
    "profileImage": "../assets/img.jpeg",
    "socialLinks": [
      { "platform": "twitter", "url": "https://twitter.com/ArjhunSreedar", "icon": "fab fa-twitter iico" },
      { "platform": "github", "url": "https://github.com/kernelism", "icon": "fab fa-github iico" },
      { "platform": "medium", "url": "https://medium.com/@arjuns0206", "icon": "fab fa-medium iico" },
      { "platform": "linkedin", "url": "https://linkedin.com/in/arjhunsreedar", "icon": "fab fa-linkedin iico" },
      { "platform": "resume", "url": "https://drive.google.com/file/d/1Rt0gJB14fGU_BRzIZKLHoL-rBRivgqLk/view?usp=sharing", "icon": "fa-solid fa-file iico-resume" }
    ]
  },
  "info": {
    "lastUpdated": "2025-12-17"
  },
  "about": {
    "title": "about me",
    "paragraphs": [
      "Hi! I'm Arjhun, a software engineer and researcher currently pursuing my Master's in Computer Science at UMass Amherst. I specialize in suddenly wanting to build something overnight, and losing interest the very next day.",
      "When I'm not busy abandoning yesterday's brilliant idea, I build things that matter. I've professionally worked in climate tech and cloud domains.",
      "Outside of code, I moonlight as a tech writer, open-source contributor, gym rat and occasional cricketer.",
      "Right now I'm deep in the rabbit hole of LLMs (aren't we all?) and how they can solve relevant problems. Always up to collaborate on interesting stuff! Hit me up at <a href=\"mailto:asreedar@umass.edu\">asreedar@umass.edu</a> or <a href=\"mailto:arjunsreedar26@gmail.com\">arjunsreedar26@gmail.com</a>."
    ]
  },
  "history": {
    "entries": [
    {
        "timespan": "2025 - Present",
        "logo": "../assets/cics.jpg",
        "description": "Working as a Graduate Researcher at <b>UMass BioNLP Lab</b>. <br/> Synthetic data generation for clinical use-cases. Working towards an EMNLP paper."
    },
      {
        "timespan": "2025 - 2027",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/UMass_Amherst_athletics_logo.svg/1200px-UMass_Amherst_athletics_logo.svg.png",
        "description": "Pursuing my <strong>Master of Science in Computer Science</strong> at University of Massachusetts, Amherst. <br/> Coursework: <ul><li>Advanced Natural Language Processing</li><li>Distributed Computing and Systems</li><li>Independent Study on NLPxBioInformatics</li></ul>"
      },
      {
        "timespan": "2024 - 2025",
        "logo": "https://1000logos.net/wp-content/uploads/2017/04/Oracle-Logo.jpg",
        "description": "<strong>Software Engineer I at Oracle</strong>, India. <br/> Modernized logging and observability with structured events and AI-assisted log triage, reducing debugging time by <b>30%</b> and mean-time-to-identify by <b>45%</b>. Improved alerting and real-time telemetry to suppress noise and increase on-call signal-to-noise by <b>60%</b>."
      },
      {
        "timespan": "2024",
        "logo": "../assets/cmhq.jpeg",
        "description": "<strong>Software Engineer Intern</strong> at <a href=\"https://carbonmarketshq.com/\">CarbonMarketsHQ</a>, India (Feb - Jul 2024). <br/> <ul><li>Stateless, concurrent and fault-tolerant data pipelining to extract, store, index and repurpose climate records.</li><li>AI climate analytics.</li><li>Maintained <b>FastAPI + GraphQL</b> backend.</li><li>Setup and maintained <b>Azure</b> hosting, <b>Postgres</b> database and AI search services.</li></ul>"
      },
      {
        "timespan": "2023-2024",
        "logo": "../assets/cmhq.jpeg",
        "description": "<strong>Software Engineer Co-op</strong> at <a href=\"https://carbonmarketshq.com/\">CarbonMarketsHQ</a>, India (Sep - Dec 2023). <br/> <ul><li>Worked directly for MicroEnergy Credits (MEC) to build and end-to-end pipeline that converts obtuse XLSX credit offset records to readable data to draw insights from.</li></ul>"
      },
      {
        "timespan": "2022 - 2023",
        "logo": "../assets/sony.jpg",
        "description": "Worked as a <strong>Research Intern</strong> at <a href=\"https://drive.google.com/file/d/1h1dA4V0fb1tIXs5nfz4UNcSm_Hv6giKR/view?usp=sharing\"> Sony SSUP </a> <br/> Efficient polyhouse based solar drying through <b>machine learning</b> and IoT."
      },
      {
        "timespan": "2022",
        "logo": "../assets/zobyt.jpeg",
        "description": "<strong>Software Engineer Intern</strong> at <a href=\"https://zobyt.com\">Zobyt</a>. <br/> <ul><li>Worked on 40+ features including carpooling, activities & itinerary planning, payment, etc on their <b>FastAPI</b> + <b>PostgreSQL backend</b>.</li><li>Wrote Extensive unit tests.</li><li>Setup <b>Stripe</b> payment end-to-end.</li></ul>"
      },
      {
        "timespan": "2020 - 2024",
        "logo": "https://www.facultyplus.com/wp-content/uploads/2021/12/Amrita-vishwa-vidyapeetham-color-logo.png",
        "description": "<strong>B.Tech in Computer Science and Engineering</strong> at Amrita Vishwa Vidyapeetham, Coimbatore. Really fell in love with CS during my time there, but WILL not endorse the curriculum, grading or values this university stands for."
      }
    ]
  },
  "writing": {
    "title": "writing",
    "subtitle": "I occasionally write on medium.",
    "articles": [
      { "date": "Feb 2025", "title": "GraphSAGE: Full Paper Walkthrough!", "url": "https://medium.com/@MinatoNamikaze02/graphsage-full-paper-walkthrough-1b0ebd79afb4" },
      { "date": "Jul 2023", "title": "Dynamic Programming I: A beginners guide.", "url": "https://medium.com/@MinatoNamikaze02/the-right-way-of-understanding-dynamic-programming-fad6a5bdafcf" },
      { "date": "Feb 2025", "title": "Let's build an LSTM from scratch.", "url": "https://medium.com/@MinatoNamikaze02/lets-build-an-lstm-from-scratch-6845e0d64dae" },
      { "date": "Jun 2025", "title": "Getting the most out of cursor/windsurf for modern frontend dev", "url": "https://medium.com/@MinatoNamikaze02/getting-the-most-out-of-cursor-windsurf-for-modern-frontend-dev-61e2559d08de" },
      { "date": "Jun 2025", "title": "Dynamic Programming II: Memoization", "url": "https://medium.com/@MinatoNamikaze02/dynamic-programming-ii-memoization-e6e9170a8fb5" },
      { "date": "Aug 2024", "title": "Modern Backend Stack Walkthrough: FastAPI, GraphQL, and Postgres", "url": "https://medium.com/@MinatoNamikaze02/a-f-guide-to-fastapi-graphql-postgres-084358d59847" },
      { "date": "Nov 2023", "title": "Making Apache Pig Output Human-Readable", "url": "https://medium.com/@MinatoNamikaze02/how-to-pretty-print-apache-pig-outputs-5c4d6c430e4c" },
      { "date": "Nov 2023", "title": "Running Selenium on Google Colab", "url": "https://medium.com/@MinatoNamikaze02/running-selenium-on-google-colab-a118d10ca5f8" },
      { "date": "Sep 2023", "title": "Installing Hadoop on MacOS (M1/M2) : 2023", "url": "https://medium.com/@MinatoNamikaze02/installing-hadoop-on-macos-m1-m2-2023-d963abeab38e" },
      { "date": "Sep 2023", "title": "Reddit Scraping Post-API Apocalypse", "url": "https://medium.com/@MinatoNamikaze02/you-dont-need-the-reddit-api-to-acquire-its-data-here-s-how-41ef8f15e1db" },
      { "date": "Jul 2023", "title": "Writing Maintainable CSS in React Projects", "url": "https://medium.com/@MinatoNamikaze02/complicating-yet-simplifying-reactjs-a51d16c0235a" },
      { "date": "Jul 2023", "title": "Mastering Named Entity Recognition with Custom Entities", "url": "https://medium.com/@MinatoNamikaze02/mastering-named-entity-recognition-with-custom-entities-5de246bbf584" },
      { "date": "Aug 2022", "title": "Conquering Lazy Loading using Selenium", "url": "https://medium.com/@MinatoNamikaze02/conquering-lazy-loading-using-selenium-c722cd44f19" },
      { "date": "Jul 2022", "title": "How to Scrape Image Data at Scale with Scrapy", "url": "https://medium.com/@MinatoNamikaze02/web-scraping-images-efficiently-using-scrapy-7902ceeab171" }
    ],
    "contentLink": {
      "text": "→ Curations",
      "url": "content.html"
    }
  },
  "projects": {
    "title": "projects",
    "items": [
      {
        "name": "yesreply",
        "url": "https://github.com/kernelism/yes-reply",
        "image": "https://img.freepik.com/free-psd/phone-icon-design_23-2151311652.jpg?semt=ais_hybrid&w=740&q=80",
        "description": "is an inbox-as-a-marketplace application that adds a paywall to your email address. Full stack application build with React.js, FastAPI, AWS SES, Cloudflare, RabbitMQ and Stripe.",
        "detailedDescription": "YesReply is a comprehensive monetized email platform designed to transform your inbox into a marketplace. The system enables users to add a paywall to their email address, allowing them to earn money from unsolicited messages while maintaining full control over who can contact them.<br/><br/><strong>Architecture:</strong> The platform uses an event-driven, fault-tolerant pipeline that processes incoming emails through AWS SES, routes them via SNS to Cloudflare Workers for preprocessing, then through RabbitMQ for reliable message queuing to the backend.<br/><br/><strong>Key Features:</strong><ul><li>Strict RFC 822 email threading compliance for proper conversation management</li><li>Idempotent email ingestion to prevent duplicate processing</li><li>Secure payment flows integrated with Stripe</li><li>JWT-based authentication with wallet credit system</li><li>Cashout functionality for earned credits</li><li>React.js frontend with real-time updates</li></ul><br/><strong>Technical Highlights:</strong> The email paywall pipeline implements exactly-once delivery semantics, ensuring no message loss while preventing duplicates. The system handles multiple failure scenarios including email provider outages, payment processing failures, and network interruptions. The implementation demonstrates deep understanding of distributed systems, event processing, and payment gateway integration."
      },
      {
        "name": "jargon",
        "url": "https://github.com/kernelism/jargon",
        "image": "https://usercentrics.com/wp-content/uploads/2021/12/blog-hero-privacy-policy.jpeg",
        "description": "is a NER based privacy policy analyser which identifies sections of importance in large jargons of text based on predefined tags generated by LLMs",
        "detailedDescription": "Jargon is an advanced privacy policy analysis tool that leverages BERT-based Named Entity Recognition (NER) to automatically extract and classify critical clauses from dense legal documents.<br/><br/><strong>Problem Solved:</strong> Privacy policies are notoriously difficult to parse manually. This tool automates the identification of important sections, categorizing data collection practices, retention policies, sharing clauses, and legal rights.<br/><br/><strong>Technical Implementation:</strong><ul><li>BERT-based clause extraction pipeline fine-tuned for legal domain</li><li>Self-training approach enables learning from limited labeled data</li><li>Achieved 0.80 F1 score on dense legal text classification</li><li>Data-efficient learning through semi-supervised techniques</li><li>Model optimization for production deployment</li></ul><br/><strong>Key Metrics:</strong> The model demonstrates strong performance on legal domain NER with careful handling of domain-specific terminology and complex clause structures. The self-training approach reduced labeled data requirements by 60% while maintaining accuracy.<br/><br/><strong>Use Cases:</strong> Helpful for researchers analyzing privacy practices across companies, regulatory compliance teams, and consumers understanding their data rights."
      },
      {
        "name": "toolboxAi",
        "url": "https://github.com/kernelism/toolboxai",
        "image": "https://raw.githubusercontent.com/kernelism/toolboxai/main/assets/toolboxai.png",
        "description": "is a web tool to ask questions about your pdf. It routes between LLMs, maintains conversation history and can be used to ask questions about your pdf, or even specific sections of it.",
        "detailedDescription": "ToolboxAI is an intelligent PDF question-answering system that makes large documents searchable and interactive through natural language queries.<br/><br/><strong>Core Functionality:</strong><ul><li>Upload and parse PDFs of any size</li><li>Ask natural language questions about document content</li><li>Get contextually relevant answers with citations</li><li>Maintain full conversation history across sessions</li><li>Query specific sections or entire documents</li></ul><br/><strong>Technical Architecture:</strong> The system uses a multi-LLM routing strategy that intelligently selects the best model for each query type based on complexity, cost, and performance considerations. Backend processes PDFs into semantic chunks, creates embeddings, and implements RAG (Retrieval-Augmented Generation) for accurate answers grounded in document content.<br/><br/><strong>Advanced Features:</strong><ul><li>LLM router that balances speed vs. accuracy</li><li>Conversation memory management for context preservation</li><li>Semantic chunking for better retrieval</li><li>Source attribution showing which document sections were used</li></ul><br/><strong>Use Cases:</strong> Perfect for researchers analyzing large papers, business professionals reviewing complex contracts, students studying from textbooks, and anyone needing to extract insights from document collections."
      },
      {
        "name": "ChokeSpeare",
        "url": "https://github.com/kernelism/chokespeare",
        "image": "https://cdn.pixabay.com/photo/2024/05/26/20/52/william-shakespeare-8789475_1280.png",
        "description": "is a collection of different generative models trained on different kinds of poems and can generate poems in a mixed style.",
        "detailedDescription": "ChokeSpeare is a creative AI system that generates original poetry by leveraging multiple fine-tuned language models, each specialized in different poetic styles and traditions.<br/><br/><strong>System Design:</strong><ul><li>Multiple model implementations, each trained on specific poetry genres (sonnets, haikus, free verse, etc.)</li><li>Style blending capabilities that combine characteristics from different poetic traditions</li><li>Controllable generation parameters for customization</li></ul><br/><strong>Models Included:</strong> The system contains individual models trained on various datasets including Shakespearean sonnets, contemporary poetry, classical Chinese poetry, and modern experimental verse.<br/><br/><strong>Technical Features:</strong><ul><li>Transfer learning from pre-trained language models</li><li>Fine-tuning pipelines for domain-specific poetry generation</li><li>Mixed-style generation using ensemble techniques</li><li>Configurable generation parameters (temperature, style weights, length constraints)</li><li>Evaluation metrics for poetic quality (meter, rhyme consistency, coherence)</li></ul><br/><strong>Creative Applications:</strong> Useful for poetry enthusiasts exploring new creative directions, educational tools for understanding different poetic styles, inspiration for human writers, and demonstration of how AI can engage with creative domains.<br/><br/><strong>Unique Aspect:</strong> The mixed-style generation is particularly interesting—it can create poetry that blends Shakespearean sonnet structure with modern free verse themes, or combine Eastern and Western poetic traditions."
      },
      {
        "name": "iDEA community website",
        "url": "https://github.com/IDEA-Amrita/iDEA",
        "image": "https://raw.githubusercontent.com/IDEA-Amrita/iDEA/main/public/images/logo.jpeg",
        "description": "is a pure react website for the undergrad club I co-founded. It does not use any external libraries except styled components. An unorthodox design and code structure. Find the website hosted <a href=\"https://ideacommunity.me\">here</a>.",
        "detailedDescription": "The iDEA community website is a pure React implementation of a community hub for the undergraduate innovation and development club at Amrita Vishwa Vidyapeetham.<br/><br/><strong>Philosophy:</strong> Built with a minimalist approach—using only React and styled-components for styling, with zero external UI component libraries. This constraint-driven development resulted in unique design patterns and deep React expertise.<br/><br/><strong>Key Features:</strong><ul><li>Member directory with profiles and contributions</li><li>Project showcase highlighting community work</li><li>Event management and registration</li><li>Resource library for learning materials</li><li>Community feed and activity tracking</li><li>Responsive design for all devices</li></ul><br/><strong>Technical Highlights:</strong><ul><li>Custom component library built from scratch using styled-components</li><li>State management using React Context API</li><li>Unconventional file structure optimized for scaling</li><li>Performance optimizations through React memoization and lazy loading</li><li>Deployment automation for continuous updates</li></ul><br/><strong>Design Approach:</strong> The \"unorthodox\" design structure reflects creative problem-solving around React patterns. Rather than following standard conventions, the architecture prioritizes maintainability and scalability through innovative component composition and state sharing patterns.<br/><br/><strong>Impact:</strong> Successfully serves as the central hub for a 200+ member community, hosts project archives, manages events, and provides resources for peer learning. The website demonstrates how constraints can lead to innovative solutions and deeper technical understanding."
      },
      {
        "name": "Reddit Scraper",
        "url": "https://github.com/kernelism/reddit-scraper",
        "image": "https://store-images.s-microsoft.com/image/apps.15970.14375561300249796.05fe8c27-ce9e-4144-8702-0e81e2f575b1.042364cb-b745-4796-a520-61291e2dd6b9",
        "description": "is a CLI tool that can scrape reddit content quickly using a unique checkpointing mechanism and JSON trick.",
        "detailedDescription": "Reddit Scraper is a high-performance command-line tool designed to efficiently collect Reddit data at scale, addressing challenges in web scraping reliability and performance.<br/><br/><strong>Problem Context:</strong> Reddit's API deprecation made traditional scraping approaches obsolete. This tool provides an alternative that efficiently bypasses common scraping challenges like rate limiting, IP blocking, and data loss.<br/><br/><strong>Key Features:</strong><ul><li>Fast bulk data collection from Reddit without API access</li><li>Intelligent checkpointing mechanism for resumable scrapes</li><li>Novel JSON extraction technique for reliable data parsing</li><li>Configurable filters (subreddit, date range, karma thresholds)</li><li>Output to structured formats (JSON, CSV, database)</li></ul><br/><strong>Technical Innovation:</strong> The unique checkpointing mechanism allows scraping jobs to resume from exactly where they left off, even after network failures. The 'JSON trick' cleverly extracts data from HTML by identifying embedded JSON structures, making it more robust than traditional HTML parsing.<br/><br/><strong>Performance Characteristics:</strong><ul><li>Multi-threaded downloading for parallel requests</li><li>Connection pooling and request batching</li><li>Efficient memory usage for large-scale scrapes</li><li>Minimal bandwidth overhead</li></ul><br/><strong>Use Cases:</strong> Academic research on Reddit communities, sentiment analysis of public discourse, market research, content archival, and social science studies. The tool democratizes access to Reddit data for researchers who lack API access."
      },
      {
        "name": "WeisfeilerLehman Quick Compute",
        "url": "https://github.com/kernelism/WL-split-merge",
        "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4MfFbNhIC5lSmBxnSAnsESSjZ6YBykM41AQ&s",
        "description": "is a python script that can speed up WL Kernel computations by computing as batches and merging.",
        "detailedDescription": "This project optimizes Weisfeiler-Lehman (WL) kernel computations, which are fundamental algorithms in graph neural networks and graph machine learning for computing graph similarity and generating graph features.<br/><br/><strong>Problem Solved:</strong> WL kernel computations on large graph datasets are computationally expensive. The naive implementation processes graphs sequentially, leading to significant bottlenecks in batch processing scenarios.<br/><br/><strong>Solution Overview:</strong> The split-merge approach decomposes large batches of graphs into independent subsets, computes WL kernels in parallel, and intelligently merges results using mathematical properties of kernel matrices.<br/><br/><strong>Technical Details:</strong><ul><li>Batch-wise kernel computation enabling parallelization</li><li>Memory-efficient merge strategy using kernel matrix algebra</li><li>Significant speedup for large-scale graph learning tasks</li><li>Maintains numerical precision despite split operations</li></ul><br/><strong>Performance Gains:</strong><ul><li>2-4x speedup on typical batches (depending on hardware)</li><li>Linear scaling with batch size</li><li>Reduced memory footprint through streaming computation</li></ul><br/><strong>Applications:</strong> Used in graph classification tasks, molecular property prediction, protein structure analysis, social network analysis, and any domain where WL kernels are applied to multiple graphs.<br/><br/><strong>Mathematical Foundation:</strong> Leverages properties of positive semi-definite matrices and kernel algebra to ensure that split-compute-merge preserves kernel matrix properties essential for downstream ML algorithms."
      },
      {
        "name": "SuitUP",
        "url": "https://github.com/kernelism/suitup",
        "image": "../assets/suitup.png",
        "description": "is a AI Application that automatically scrapes jobs relevant to your resume.",
        "detailedDescription": "SuitUP is an intelligent job discovery platform that leverages AI and web scraping to match you with job opportunities tailored to your skills and experience.<br/><br/><strong>Problem Solved:</strong> Job hunting is tedious—manually browsing job boards, filtering by skills, and applying to irrelevant positions wastes time. SuitUP automates this process by analyzing your resume and proactively finding positions that align with your expertise.<br/><br/><strong>Core Features:</strong><ul><li>Resume parsing to extract skills, experience, and qualifications</li><li>Intelligent job scraping from multiple job boards and company websites</li><li>AI-powered matching algorithm to rank jobs by relevance</li><li>Automated filtering based on your preferences (location, salary, company type)</li><li>Job aggregation from diverse sources into a unified dashboard</li><li>Application tracking and job history management</li></ul><br/><strong>Technical Architecture:</strong><ul><li>Resume parsing using NLP and PDF extraction</li><li>Distributed web scrapers for concurrent job collection across multiple platforms</li><li>LLM-based job relevance scoring comparing resume content with job descriptions</li><li>Efficient database for storing and querying job listings</li><li>Real-time job alert system with smart deduplication</li></ul><br/><strong>Key Technologies:</strong> Python for backend logic, web scraping libraries for data collection, LLMs for intelligent matching, and modern APIs for job board integration.<br/><br/><strong>Unique Value Proposition:</strong> Unlike passive job boards that require you to search, SuitUP actively brings opportunities to you. The AI-driven matching goes beyond simple keyword matching—it understands context, experience progression, and genuine skill compatibility.<br/><br/><strong>User Impact:</strong> Reduces job search time by automatically filtering noise and surfacing only highly relevant opportunities. Helps job seekers discover roles they might have missed and increases interview-to-application ratios."
      }
    ]
  },
  "publications": {
    "title": "publications",
    "items": [
      {
        "title": "Content-Agnostic Community Classification Using Meta-Graph Representations of Conversational Dynamics - Preprint",
        "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5238558",
        "venue": "OSNEM 2025 (In peer-review)",
        "authors": "Arjhun Sreedar, Pranav Deepak, Dayanand V"
      }
    ]
  },
  "misc": {
    "title": "more links",
    "items": [
      {
        "title": "medium.com",
        "description": "",
        "url": "https://medium.com/@MinatoNamikaze02"
      },
      {
        "title": "chess.com",
        "description": "",
        "url": "https://www.chess.com/member/trippitroppi69"
      },
      {
        "title": "letterboxd",
        "description": "",
        "url": "https://letterboxd.com/movietourist02/"
      },
      {
        "title": "leetcode",
        "description": "",
        "url": "https://leetcode.com/u/kernelism/"
      },
      {
        "title": "codeforces",
        "description": "",
        "url": "https://codeforces.com/profile/kernelism"
      },
      {
        "title": "youtube",
        "description": "",
        "url": "https://www.youtube.com/watch?v=xvFZjo5PgG0"
      },
      {
        "title": "discord",
        "description": "arjuns02",
        "url": ""
      }
    ]
  },
  "contact": {
    "title": "contact",
    "description": "Have something interesting to discuss? Drop me a message.",
    "placeholders": {
      "name": "Name",
      "email": "Email",
      "message": "Message"
    },
    "submitButton": "Send",
    "submitButtonSending": "Sending...",
    "messages": {
      "fillAllFields": "Please fill in all fields.",
      "invalidEmail": "Please enter a valid email address.",
      "success": "Message sent successfully! I'll get back to you soon.",
      "error": "Failed to send message. Please try again or email me directly."
    }
  },
  "footer": {
    "attribution": "Base design adapted from <a href=\"https://karpathy.ai/\" style=\"color: var(--text-muted); transition: color 0.3s ease;\">Andrej Karpathy's</a> portfolio.",
    "tagline": "Less, but better."
  }
}
